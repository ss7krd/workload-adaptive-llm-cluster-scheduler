cache_config_block_size: 16
cache_config_num_gpu_blocks: null
enable_profiling: false
faster_transformer_scheduler_config_max_num_seqs: 128
fixed_request_length_generator_config_decode_tokens: 512
fixed_request_length_generator_config_prefill_tokens: 4096
fixed_request_length_generator_config_seed: 42
gamma_request_interval_generator_config_cv: 0.5
gamma_request_interval_generator_config_qps: 1.0
gamma_request_interval_generator_config_seed: 42
interval_generator_config_type: !!python/object/apply:sarathi.types.RequestIntervalGeneratorType
- POISSON
length_generator_config_type: !!python/object/apply:sarathi.types.RequestLengthGeneratorType
- FIXED
log_level: info
metrics_config_enable_chrome_trace: true
metrics_config_enable_cpu_op_level_metrics: false
metrics_config_enable_op_level_metrics: false
metrics_config_enable_request_outputs: false
metrics_config_keep_individual_batch_metrics: false
metrics_config_wandb_group: null
metrics_config_wandb_project: null
metrics_config_wandb_run_id: null
metrics_config_wandb_run_name: null
metrics_config_wandb_sweep_id: null
metrics_config_write_metrics: true
model_config_download_dir: null
model_config_dtype: float16
model_config_load_format: auto
model_config_max_model_len: null
model_config_model: meta-llama/Meta-Llama-3-8B-Instruct
model_config_revision: null
model_config_seed: 0
model_config_trust_remote_code: true
num_replicas: 2
orca_scheduler_config_max_num_seqs: 128
output_dir: benchmark_output
parallel_config_pipeline_parallel_size: 1
parallel_config_tensor_parallel_size: 1
poisson_request_interval_generator_config_qps: 1.0
poisson_request_interval_generator_config_seed: 42
replica_resource_mapping: null
request_generator_config_type: !!python/object/apply:sarathi.types.RequestGeneratorType
- SYNTHETIC
sarathi_scheduler_config_chunk_schedule_max_tokens: null
sarathi_scheduler_config_chunk_schedule_stages: null
sarathi_scheduler_config_chunk_size: 512
sarathi_scheduler_config_enable_dynamic_chunking_schedule: false
sarathi_scheduler_config_high_chunk_size: null
sarathi_scheduler_config_low_chunk_size: null
sarathi_scheduler_config_max_num_seqs: 128
scheduler_config_type: !!python/object/apply:sarathi.types.SchedulerType
- SARATHI
seed: 42
simple_chunking_scheduler_config_chunk_size: 512
simple_chunking_scheduler_config_max_num_seqs: 128
static_request_interval_generator_config_seed: 42
synthetic_request_generator_config_duration: null
synthetic_request_generator_config_num_requests: 64
synthetic_request_generator_config_seed: 42
time_limit: null
trace_request_generator_config_date: '2023-08-21'
trace_request_generator_config_decode_scale_factor: 1
trace_request_generator_config_max_tokens: 4096
trace_request_generator_config_prefill_scale_factor: 0.3
trace_request_generator_config_seed: 42
trace_request_generator_config_time_scale_factor: 0.04
trace_request_generator_config_trace_file: data/processed_traces/sydney_enterprise.csv
trace_request_interval_generator_config_end_time: '1970-01-04 15:00:00'
trace_request_interval_generator_config_seed: 42
trace_request_interval_generator_config_start_time: '1970-01-04 12:00:00'
trace_request_interval_generator_config_time_scale_factor: 0.3
trace_request_interval_generator_config_trace_file: data/processed_traces/AzureFunctionsInvocationTraceForTwoWeeksJan2021Processed.csv
trace_request_length_generator_config_decode_scale_factor: 1
trace_request_length_generator_config_max_tokens: 4096
trace_request_length_generator_config_prefill_scale_factor: 1
trace_request_length_generator_config_seed: 42
trace_request_length_generator_config_trace_file: data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv
uniform_request_length_generator_config_max_tokens: 4096
uniform_request_length_generator_config_min_tokens: 1024
uniform_request_length_generator_config_prefill_to_decode_ratio: 20.0
uniform_request_length_generator_config_seed: 42
vllm_scheduler_config_max_batched_tokens: null
vllm_scheduler_config_max_num_seqs: 128
worker_config_attention_backend: !!python/object/apply:sarathi.types.AttentionBackend
- FLASHINFER
worker_config_gpu_memory_utilization: 0.8
write_json_trace: true
zipf_request_length_generator_config_max_tokens: 4096
zipf_request_length_generator_config_min_tokens: 1024
zipf_request_length_generator_config_prefill_to_decode_ratio: 20.0
zipf_request_length_generator_config_scramble: false
zipf_request_length_generator_config_seed: 42
zipf_request_length_generator_config_theta: 0.6
